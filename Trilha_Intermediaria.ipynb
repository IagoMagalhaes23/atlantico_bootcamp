{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPYUG24AjAuTV4r6KD2Uf0q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IagoMagalhaes23/atlantico_bootcamp/blob/main/Trilha_Intermediaria.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Importação bibliotecas"
      ],
      "metadata": {
        "id": "22iAT5iTMeds"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzGGG7r-MVTa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from random import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics,svm\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn import tree\n",
        "from skimage.measure import regionprops\n",
        "from skimage.filters import threshold_otsu\n",
        "from sklearn.preprocessing import MaxAbsScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preparando o dataset"
      ],
      "metadata": {
        "id": "g0XPMzqOMl3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(datadir, classes, img_size=100):\n",
        "    training_data = []\n",
        "    label = []\n",
        "    for classe in range(len(classes)):\n",
        "        path = os.path.join(datadir, classes[classe])\n",
        "        shufled_list  = list(os.listdir(path))\n",
        "        shuffle(shufled_list)\n",
        "        for img in shufled_list:\n",
        "            img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
        "            img_array = cv2.resize(img_array, (img_size, img_size))\n",
        "            unique = np.unique(img_array)\n",
        "            if len(unique) == 1:\n",
        "                continue\n",
        "            training_data.append(img_array)\n",
        "            label.append(classe)\n",
        "    return training_data , label"
      ],
      "metadata": {
        "id": "j0A1HmcmMpW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data , label = load_data('dataset/geometric',['circle','square','star','triangle'])"
      ],
      "metadata": {
        "id": "M_uN13PNMsTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extração de atributos"
      ],
      "metadata": {
        "id": "HKZN1AKdMvNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_contours_param(contour):\n",
        "    contour_area = contour[0].filled_area\n",
        "    contour_perimeter = contour[0].perimeter\n",
        "    contour_convex_area = contour[0].convex_area\n",
        "    diameter = contour[0].equivalent_diameter\n",
        "    return contour_area , contour_perimeter, contour_convex_area, diameter"
      ],
      "metadata": {
        "id": "Zz72mwzlMzQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def features_extraction(images):\n",
        "    features_list = []\n",
        "    for image in images:\n",
        "        thresh = threshold_otsu(image)\n",
        "        binary = np.array(image > thresh).astype(int)\n",
        "        white_pixel = np.where(binary > 0)\n",
        "        if len(white_pixel[0]) > 7000:\n",
        "            binary = abs(1-binary) # ajuste de imagens negativas\n",
        "        regions = regionprops(binary)\n",
        "        contour_area , contour_perimeter, contour_convex_area, diameter = get_contours_param(regions)\n",
        "        features_list.append([contour_area , contour_perimeter, contour_convex_area, diameter])\n",
        "    norm =  MaxAbsScaler()\n",
        "    norm.fit(features_list)\n",
        "    norm_features = norm.transform(features_list)\n",
        "    return norm_features"
      ],
      "metadata": {
        "id": "NxtT8hHyM11U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = features_extraction(data)"
      ],
      "metadata": {
        "id": "Lgz9WxEHM6Ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Treinamento e Teste dos Classificadores"
      ],
      "metadata": {
        "id": "4PMPXY25M67s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_svm_model(train_data,label_train_data,test_data):\n",
        "    clf = svm.SVC(kernel='linear')\n",
        "    clf.fit(train_data, label_train_data)\n",
        "    predicted = clf.predict(test_data)\n",
        "    return predicted\n",
        "def generate_SGDC_model(train_data,label_train_data,test_data):\n",
        "    clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=200)\n",
        "    clf.fit(train_data, label_train_data)\n",
        "    predicted = clf.predict(test_data)\n",
        "    return predicted\n",
        "def generate_naive_bayes_model(train_data,label_train_data,test_data):\n",
        "    gnb = GaussianNB()\n",
        "    gnb.fit(train_data, label_train_data)\n",
        "    predicted = gnb.predict(test_data)\n",
        "    return predicted\n",
        "def generate_decision_tree_model(train_data,label_train_data,test_data):\n",
        "    clf = tree.DecisionTreeClassifier()\n",
        "    clf = clf.fit(train_data, label_train_data)\n",
        "    predicted = clf.predict(test_data)\n",
        "    return predicted\n",
        "def generate_random_forest_model(X_train, y_train,test_data):\n",
        "    rfc = RandomForestClassifier(criterion= 'entropy', max_depth= 8, max_features='auto', n_estimators=200)\n",
        "    rfc.fit(X_train,y_train)\n",
        "    predicted = rfc.predict(test_data)\n",
        "    return predicted\n",
        "def generate_MLP_model(X_train, y_train,test_data):\n",
        "    classifier = MLPClassifier(hidden_layer_sizes=(100,100,100), max_iter=300,activation = 'relu',solver='adam',random_state=1)\n",
        "    classifier.fit(X_train, y_train)\n",
        "    predicted = classifier.predict(test_data)\n",
        "    return predicted\n",
        "def generate_knn_model(train_data,label_train_data,test_data):\n",
        "    knn = KNeighborsClassifier()\n",
        "    knn.fit(train_data,label_train_data)\n",
        "    predicted = knn.predict(test_data)\n",
        "    return predicted"
      ],
      "metadata": {
        "id": "rCxdWeomM-8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_classifiers(train_data,label_train_data,test_data):\n",
        "    return generate_knn_model(train_data,label_train_data,test_data),\\\n",
        "    generate_MLP_model(train_data,label_train_data,test_data),\\\n",
        "    generate_SGDC_model(train_data,label_train_data,test_data),\\\n",
        "    generate_svm_model(train_data,label_train_data,test_data),\\\n",
        "    generate_decision_tree_model(train_data,label_train_data,test_data),\\\n",
        "    generate_naive_bayes_model(train_data,label_train_data,test_data),\\\n",
        "    generate_random_forest_model(train_data,label_train_data,test_data),"
      ],
      "metadata": {
        "id": "jdL293X_NCfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test,y_train,y_test = train_test_split(features,label,test_size=0.3)\n",
        "results = gen_classifiers(X_train, y_train,X_test)"
      ],
      "metadata": {
        "id": "GerOHz1dNF6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Avaliação dos classificadores"
      ],
      "metadata": {
        "id": "BhI7RHuFNKpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = metrics.accuracy_score(test_labels,predicted)"
      ],
      "metadata": {
        "id": "9UgrO5RsNL8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recall = metrics.recall_score(y_test,results[0],average=None)"
      ],
      "metadata": {
        "id": "tGNGLTBXNOrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision = metrics.precision_score(y_test,results[0],average=None)"
      ],
      "metadata": {
        "id": "mRUiz-lxNQqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "precision = metrics.precision_score(y_test,results[0],average=None)"
      ],
      "metadata": {
        "id": "Ep2vROhWNSII"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}